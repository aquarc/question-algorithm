{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0c2a3bb4",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-04-07T03:23:05.823852Z",
     "iopub.status.busy": "2025-04-07T03:23:05.823427Z",
     "iopub.status.idle": "2025-04-07T03:23:06.845294Z",
     "shell.execute_reply": "2025-04-07T03:23:06.844088Z"
    },
    "papermill": {
     "duration": 1.031657,
     "end_time": "2025-04-07T03:23:06.847467",
     "exception": false,
     "start_time": "2025-04-07T03:23:05.815810",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68df2bbb",
   "metadata": {
    "papermill": {
     "duration": 0.00498,
     "end_time": "2025-04-07T03:23:06.858350",
     "exception": false,
     "start_time": "2025-04-07T03:23:06.853370",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Some parts of this Codelab are (c) Google 2025 under the Apache License.\n",
    "(c) Aquarc 2025"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1682779",
   "metadata": {
    "papermill": {
     "duration": 0.004684,
     "end_time": "2025-04-07T03:23:06.868216",
     "exception": false,
     "start_time": "2025-04-07T03:23:06.863532",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Synopsis\n",
    "Aquarc is an all-in-one SAT platform for high schoolers designed to minimize time spent using the software and maximizing practice and essential questions. In order to further this mission, Aquarc Intelligence was created to analyze mistakes within a question and to suggest similar questions for efficient practicing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d92a75b",
   "metadata": {
    "papermill": {
     "duration": 0.004578,
     "end_time": "2025-04-07T03:23:06.877757",
     "exception": false,
     "start_time": "2025-04-07T03:23:06.873179",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Install the SDK \n",
    "We will be using Google's Gemini and utilities to build the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ac48249a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-07T03:23:06.889352Z",
     "iopub.status.busy": "2025-04-07T03:23:06.888707Z",
     "iopub.status.idle": "2025-04-07T03:23:16.534619Z",
     "shell.execute_reply": "2025-04-07T03:23:16.533148Z"
    },
    "papermill": {
     "duration": 9.654297,
     "end_time": "2025-04-07T03:23:16.536905",
     "exception": false,
     "start_time": "2025-04-07T03:23:06.882608",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m144.7/144.7 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m100.9/100.9 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25h"
     ]
    }
   ],
   "source": [
    "!pip uninstall -qqy jupyterlab  # Remove unused packages from Kaggle's base image that conflict\n",
    "!pip install -U -q \"google-genai==1.7.0\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e014df7",
   "metadata": {
    "papermill": {
     "duration": 0.004915,
     "end_time": "2025-04-07T03:23:16.547419",
     "exception": false,
     "start_time": "2025-04-07T03:23:16.542504",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Import the SDK and set up the API key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3ff4136c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-07T03:23:16.558900Z",
     "iopub.status.busy": "2025-04-07T03:23:16.558431Z",
     "iopub.status.idle": "2025-04-07T03:23:17.886895Z",
     "shell.execute_reply": "2025-04-07T03:23:17.885876Z"
    },
    "papermill": {
     "duration": 1.336803,
     "end_time": "2025-04-07T03:23:17.889183",
     "exception": false,
     "start_time": "2025-04-07T03:23:16.552380",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from google import genai\n",
    "from google.genai import types\n",
    "\n",
    "from IPython.display import HTML, Markdown, display"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecac69b8",
   "metadata": {
    "papermill": {
     "duration": 0.005041,
     "end_time": "2025-04-07T03:23:17.900224",
     "exception": false,
     "start_time": "2025-04-07T03:23:17.895183",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Set up a retry helper so we can press \"Run All\" and not worry about hitting the quota. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "18552fcb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-07T03:23:17.912886Z",
     "iopub.status.busy": "2025-04-07T03:23:17.912138Z",
     "iopub.status.idle": "2025-04-07T03:23:18.151651Z",
     "shell.execute_reply": "2025-04-07T03:23:18.150448Z"
    },
    "papermill": {
     "duration": 0.248331,
     "end_time": "2025-04-07T03:23:18.153833",
     "exception": false,
     "start_time": "2025-04-07T03:23:17.905502",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from google.api_core import retry\n",
    "\n",
    "\n",
    "is_retriable = lambda e: (isinstance(e, genai.errors.APIError) and e.code in {429, 503})\n",
    "\n",
    "genai.models.Models.generate_content = retry.Retry(\n",
    "    predicate=is_retriable)(genai.models.Models.generate_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ff7c481",
   "metadata": {
    "papermill": {
     "duration": 0.005106,
     "end_time": "2025-04-07T03:23:18.164701",
     "exception": false,
     "start_time": "2025-04-07T03:23:18.159595",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Set up your API key\n",
    "\n",
    "To run the following cell, your API key must be stored it in a [Kaggle secret](https://www.kaggle.com/discussions/product-feedback/114053) named `GOOGLE_API_KEY`.\n",
    "\n",
    "If you don't already have an API key, you can grab one from [AI Studio](https://aistudio.google.com/app/apikey). You can find [detailed instructions in the docs](https://ai.google.dev/gemini-api/docs/api-key).\n",
    "\n",
    "To make the key available through Kaggle secrets, choose `Secrets` from the `Add-ons` menu and follow the instructions to add your key or enable it for this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d5919980",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-07T03:23:18.177103Z",
     "iopub.status.busy": "2025-04-07T03:23:18.176422Z",
     "iopub.status.idle": "2025-04-07T03:23:18.264134Z",
     "shell.execute_reply": "2025-04-07T03:23:18.262798Z"
    },
    "papermill": {
     "duration": 0.096557,
     "end_time": "2025-04-07T03:23:18.266658",
     "exception": false,
     "start_time": "2025-04-07T03:23:18.170101",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from kaggle_secrets import UserSecretsClient\n",
    "\n",
    "GOOGLE_API_KEY = UserSecretsClient().get_secret(\"GOOGLE_API_KEY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7755f2c",
   "metadata": {
    "papermill": {
     "duration": 0.005079,
     "end_time": "2025-04-07T03:23:18.277441",
     "exception": false,
     "start_time": "2025-04-07T03:23:18.272362",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Choose your model\n",
    "Depending on what's available and your quota, choose a model that's effective for your purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "16ba10ea",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-07T03:23:18.289435Z",
     "iopub.status.busy": "2025-04-07T03:23:18.288973Z",
     "iopub.status.idle": "2025-04-07T03:23:18.783382Z",
     "shell.execute_reply": "2025-04-07T03:23:18.782091Z"
    },
    "papermill": {
     "duration": 0.502923,
     "end_time": "2025-04-07T03:23:18.785488",
     "exception": false,
     "start_time": "2025-04-07T03:23:18.282565",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "models/chat-bison-001\n",
      "models/text-bison-001\n",
      "models/embedding-gecko-001\n",
      "models/gemini-1.0-pro-vision-latest\n",
      "models/gemini-pro-vision\n",
      "models/gemini-1.5-pro-latest\n",
      "models/gemini-1.5-pro-001\n",
      "models/gemini-1.5-pro-002\n",
      "models/gemini-1.5-pro\n",
      "models/gemini-1.5-flash-latest\n",
      "models/gemini-1.5-flash-001\n",
      "models/gemini-1.5-flash-001-tuning\n",
      "models/gemini-1.5-flash\n",
      "models/gemini-1.5-flash-002\n",
      "models/gemini-1.5-flash-8b\n",
      "models/gemini-1.5-flash-8b-001\n",
      "models/gemini-1.5-flash-8b-latest\n",
      "models/gemini-1.5-flash-8b-exp-0827\n",
      "models/gemini-1.5-flash-8b-exp-0924\n",
      "models/gemini-2.5-pro-exp-03-25\n",
      "models/gemini-2.5-pro-preview-03-25\n",
      "models/gemini-2.0-flash-exp\n",
      "models/gemini-2.0-flash\n",
      "models/gemini-2.0-flash-001\n",
      "models/gemini-2.0-flash-exp-image-generation\n",
      "models/gemini-2.0-flash-lite-001\n",
      "models/gemini-2.0-flash-lite\n",
      "models/gemini-2.0-flash-lite-preview-02-05\n",
      "models/gemini-2.0-flash-lite-preview\n",
      "models/gemini-2.0-pro-exp\n",
      "models/gemini-2.0-pro-exp-02-05\n",
      "models/gemini-exp-1206\n",
      "models/gemini-2.0-flash-thinking-exp-01-21\n",
      "models/gemini-2.0-flash-thinking-exp\n",
      "models/gemini-2.0-flash-thinking-exp-1219\n",
      "models/learnlm-1.5-pro-experimental\n",
      "models/gemma-3-1b-it\n",
      "models/gemma-3-4b-it\n",
      "models/gemma-3-12b-it\n",
      "models/gemma-3-27b-it\n",
      "models/embedding-001\n",
      "models/text-embedding-004\n",
      "models/gemini-embedding-exp-03-07\n",
      "models/gemini-embedding-exp\n",
      "models/aqa\n",
      "models/imagen-3.0-generate-002\n"
     ]
    }
   ],
   "source": [
    "client = genai.Client(api_key=GOOGLE_API_KEY)\n",
    "\n",
    "for model in client.models.list():\n",
    "  print(model.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92268c45",
   "metadata": {
    "papermill": {
     "duration": 0.005264,
     "end_time": "2025-04-07T03:23:18.796599",
     "exception": false,
     "start_time": "2025-04-07T03:23:18.791335",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Check out the detailed information for your model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "24ea39fa",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-07T03:23:18.809105Z",
     "iopub.status.busy": "2025-04-07T03:23:18.808591Z",
     "iopub.status.idle": "2025-04-07T03:23:18.953832Z",
     "shell.execute_reply": "2025-04-07T03:23:18.952243Z"
    },
    "papermill": {
     "duration": 0.153849,
     "end_time": "2025-04-07T03:23:18.955834",
     "exception": false,
     "start_time": "2025-04-07T03:23:18.801985",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'name': 'models/gemini-2.0-flash', 'display_name': 'Gemini 2.0 Flash', 'description': 'Gemini 2.0 Flash', 'version': '2.0', 'tuned_model_info': {}, 'input_token_limit': 1048576, 'output_token_limit': 8192, 'supported_actions': ['generateContent', 'countTokens']}\n"
     ]
    }
   ],
   "source": [
    "for model in client.models.list():\n",
    "  if model.name == 'models/gemini-2.0-flash':\n",
    "    print(model.to_json_dict())\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "745f1171",
   "metadata": {
    "papermill": {
     "duration": 0.005279,
     "end_time": "2025-04-07T03:23:18.967065",
     "exception": false,
     "start_time": "2025-04-07T03:23:18.961786",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Test your model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5ef9ea14",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-07T03:23:18.980165Z",
     "iopub.status.busy": "2025-04-07T03:23:18.979729Z",
     "iopub.status.idle": "2025-04-07T03:23:19.384170Z",
     "shell.execute_reply": "2025-04-07T03:23:19.382612Z"
    },
    "papermill": {
     "duration": 0.413486,
     "end_time": "2025-04-07T03:23:19.386143",
     "exception": false,
     "start_time": "2025-04-07T03:23:18.972657",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Greetings, Zlork! It's nice to meet you. Is there anything I can help you with today?\n",
      "\n"
     ]
    }
   ],
   "source": [
    "chat = client.chats.create(model='gemini-2.0-flash', history=[])\n",
    "response = chat.send_message('Hello! My name is Zlork.')\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e71c8e01",
   "metadata": {
    "papermill": {
     "duration": 0.005089,
     "end_time": "2025-04-07T03:23:19.396842",
     "exception": false,
     "start_time": "2025-04-07T03:23:19.391753",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "You can use the `Markdown()` function to format it nicely in Kaggle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1ce4bae5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-07T03:23:19.409250Z",
     "iopub.status.busy": "2025-04-07T03:23:19.408785Z",
     "iopub.status.idle": "2025-04-07T03:23:20.099738Z",
     "shell.execute_reply": "2025-04-07T03:23:20.098680Z"
    },
    "papermill": {
     "duration": 0.699632,
     "end_time": "2025-04-07T03:23:20.101685",
     "exception": false,
     "start_time": "2025-04-07T03:23:19.402053",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Ah, Zlork! A pleasure to make your acquaintance!\n",
       "\n",
       "***\n",
       "\n",
       "How may I be of *service* to you today? Perhaps some linguistic exploration? Or perhaps a foray into the digital arts? \n",
       "\n",
       "I am at your disposal. 😉\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = chat.send_message('Hello! My name is Zlork.' + \n",
    "                             'Use some fancy markdown in your message')\n",
    "Markdown(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bbc9915",
   "metadata": {
    "papermill": {
     "duration": 0.005112,
     "end_time": "2025-04-07T03:23:20.112274",
     "exception": false,
     "start_time": "2025-04-07T03:23:20.107162",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# A fine prompt\n",
    "Let's start by writing our prompt. What should it include and what should it mention? Following the principles taught throughout the course, let's place an importance on giving **positive** instructions rather than **negative** instructions to maintain effectiveness. \n",
    "\n",
    "The model needs to do the following:\n",
    "- Stick to the SAT: Understand weighting and importance of certain questions, categories, and ensure all advice is applicable to the bounds of the SAT. A document with the specifications of the SAT format can be used for Retrieval Augmented Generation (RAG) rather than potential hallucination over the exact requirements.\n",
    "- Have access to the current question (and maybe the previous questions for even more context)\n",
    "- Have access to the answers and time between each to estimate confidence.\n",
    "- Understand images or SVGs for Inference and other Reading/Writing questions.\n",
    "- Use Tree of Thoughts (ToT) to generate multiple solving processes because multiple methods may be used to arrive at the same answer, and output these answering mechanisms in a JSON array to present on the website effectively (a TUI for this notebook)\n",
    "- Find semantically related questions and present them to the user on the website using ReAct (a TUI for this notebook)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a3777b6",
   "metadata": {
    "papermill": {
     "duration": 0.005036,
     "end_time": "2025-04-07T03:23:20.122587",
     "exception": false,
     "start_time": "2025-04-07T03:23:20.117551",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Let's start by finding an effective prompt. \n",
    "Then we can evaluate the effectiveness of including the SAT standards in a PDF to help the user answering a question. We will also evaluate whether a vector search database is useful for this document.\n",
    "\n",
    "Below, the variables for the specific question we are testing are defined."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d7d747f5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-07T03:23:20.134769Z",
     "iopub.status.busy": "2025-04-07T03:23:20.134279Z",
     "iopub.status.idle": "2025-04-07T03:23:20.139977Z",
     "shell.execute_reply": "2025-04-07T03:23:20.138780Z"
    },
    "papermill": {
     "duration": 0.014026,
     "end_time": "2025-04-07T03:23:20.141757",
     "exception": false,
     "start_time": "2025-04-07T03:23:20.127731",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# All questions are (c) CollegeBoard 2025.\n",
    "question = \"\"\"\n",
    "In a paper about p-i-n planar perovskite solar cells (one of several perovskite cell architectures designed to collect and store solar power), Lyndsey McMillon-Brown et al. described a method for fabricating the cell’s electronic transport layer (ETL) using a spray coating. Conventional ETL fabrication is accomplished using a solution of nanoparticles. The process can result in a loss of up to 80% of the solution, increasing the cost of manufacturing at scale—an issue that may be obviated by spray coating fabrication, which the researchers describe as “highly reproducible, concise, and practical.”\n",
    "\n",
    "What does the text most strongly suggest about conventional ETL fabrication?\n",
    "A. It is less suitable for manufacturing large volumes of planar p-i-n perovskite solar cells than an alternative fabrication method may be.\n",
    "B. It is more expensive when manufacturing at scale than are processes for fabricating ETLs used in other perovskite solar cell architectures.\n",
    "C. It typically entails a greater loss of nanoparticle solution than do other established approaches for ETL fabrication.\n",
    "D. It is somewhat imprecise and therefore limits the potential effectiveness of p-i-n planar perovskite solar cells at capturing and storing solar power.\n",
    "\"\"\"\n",
    "\n",
    "rationale = \"\"\"\n",
    "Choice A is the best answer. Conventional solar cell fabrication increases “the cost of manufacturing at scale,” but spray coating might get rid of that problem.\n",
    "\n",
    "Choice B is incorrect. This is not completely supported by the text. While it’s true that conventional ETL fabrication is expensive at scale, there’s nothing in the text that mentions other perovskite solar cell architectures. Choice C is incorrect. This choice does not match the text. Only one conventional method of ETL fabrication is described, so we can’t compare the solution loss in this method to that of other conventional methods. Choice D is incorrect. This choice isn’t supported by the text. The text never suggests that the effectiveness of solar cells changes based on their method of fabrication. \n",
    "\"\"\"\n",
    "\n",
    "user_answer = \"C\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4305d33",
   "metadata": {
    "papermill": {
     "duration": 0.005285,
     "end_time": "2025-04-07T03:23:20.152482",
     "exception": false,
     "start_time": "2025-04-07T03:23:20.147197",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Ideally, the user will talk to the chatbot after it gets the question wrong (or before in some scenarios, too). Either way, the user will have some rationale as to his or her answer to the question. Don't expect this rationale to be well thought out - the objective of the intelligent agent is to draw out what they actually mean. It may be completely omitted as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ce4c6fdf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-07T03:23:20.164718Z",
     "iopub.status.busy": "2025-04-07T03:23:20.164196Z",
     "iopub.status.idle": "2025-04-07T03:23:20.169221Z",
     "shell.execute_reply": "2025-04-07T03:23:20.167961Z"
    },
    "papermill": {
     "duration": 0.013353,
     "end_time": "2025-04-07T03:23:20.171134",
     "exception": false,
     "start_time": "2025-04-07T03:23:20.157781",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "user_rationale = \"Isn't the new method of ETL fabrication the same as the 'established methods'\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65915336",
   "metadata": {
    "papermill": {
     "duration": 0.005089,
     "end_time": "2025-04-07T03:23:20.181655",
     "exception": false,
     "start_time": "2025-04-07T03:23:20.176566",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Synthesize the prompt:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6b19b62b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-07T03:23:20.193884Z",
     "iopub.status.busy": "2025-04-07T03:23:20.193400Z",
     "iopub.status.idle": "2025-04-07T03:23:20.663123Z",
     "shell.execute_reply": "2025-04-07T03:23:20.662008Z"
    },
    "papermill": {
     "duration": 0.478115,
     "end_time": "2025-04-07T03:23:20.665022",
     "exception": false,
     "start_time": "2025-04-07T03:23:20.186907",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nchat = client.chats.create(model=\\'gemini-2.0-flash\\', history=[\\n# The prompt\\n    \"Help the user please using the specs I guess\",\\n    f\"{specs}: {question} {rationale}\\n The user got: {user_answer}\\n {user_rationale}\",\\n])\\n\\nresponse = chat.send_message(\"?\")\\nMarkdown(response.text)\\n\\n\\nresponse = client.models.generate_content(\\n    model=\\'gemini-2.0-flash\\',\\n    config=types.GenerateContentConfig(\\n        temperature=1,\\n        top_p=1,\\n        max_output_tokens=1024,\\n    ),\\n    contents=[\\n# The prompt\\n    \"Help the user please using the specs I guess\",\\n# The actual thing\\n    f\"{specs}: {question} {rationale}\\n The user got: {user_answer}\\n {user_rationale}\",\\n])\\nMarkdown(response.text)\\n'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "specs = !curl https://satsuite.collegeboard.org/media/pdf/assessment-framework-for-digital-sat-suite.pdf\n",
    "\"\"\"\n",
    "chat = client.chats.create(model='gemini-2.0-flash', history=[\n",
    "# The prompt\n",
    "    \"Help the user please using the specs I guess\",\n",
    "    f\"{specs}: {question} {rationale}\\n The user got: {user_answer}\\n {user_rationale}\",\n",
    "])\n",
    "\n",
    "response = chat.send_message(\"?\")\n",
    "Markdown(response.text)\n",
    "\n",
    "\n",
    "response = client.models.generate_content(\n",
    "    model='gemini-2.0-flash',\n",
    "    config=types.GenerateContentConfig(\n",
    "        temperature=1,\n",
    "        top_p=1,\n",
    "        max_output_tokens=1024,\n",
    "    ),\n",
    "    contents=[\n",
    "# The prompt\n",
    "    \"Help the user please using the specs I guess\",\n",
    "# The actual thing\n",
    "    f\"{specs}: {question} {rationale}\\n The user got: {user_answer}\\n {user_rationale}\",\n",
    "])\n",
    "Markdown(response.text)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "389c01e9",
   "metadata": {
    "papermill": {
     "duration": 0.005595,
     "end_time": "2025-04-07T03:23:20.676676",
     "exception": false,
     "start_time": "2025-04-07T03:23:20.671081",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "do some mathplotlib thingies"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [],
   "dockerImageVersionId": 30918,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 19.542209,
   "end_time": "2025-04-07T03:23:21.805796",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-04-07T03:23:02.263587",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
